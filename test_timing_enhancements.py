#!/usr/bin/env python3
"""
Test the enhanced timing features in reports
"""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_timing_enhancements():
    """Test the enhanced timing features"""
    print("ğŸ§ª Testing Enhanced Timing Features in Reports")
    print("=" * 70)
    
    try:
        from gitllama.utils.context_tracker import context_tracker
        from gitllama.ai.query import AIQuery
        
        # Reset tracker for clean test
        context_tracker.reset()
        context_tracker.start_stage("Test_Timing")
        
        print("âœ… Context tracker and AIQuery can be imported")
        
        # Simulate storing a prompt-response pair with timing
        variables_used = {
            "context": "Test context",
            "question": "Test question"
        }
        
        # Simulate different execution times
        test_pairs = [
            {
                "prompt": "Test multiple choice query",
                "response": "A",
                "query_type": "multiple_choice",
                "execution_time": 1.25
            },
            {
                "prompt": "Test single word query", 
                "response": "Python",
                "query_type": "single_word",
                "execution_time": 0.87
            },
            {
                "prompt": "Test open response query",
                "response": "This is a detailed response explaining the system architecture...",
                "query_type": "open", 
                "execution_time": 3.42
            },
            {
                "prompt": "Test file write query",
                "response": "def main():\n    print('Hello World!')\n    return 0",
                "query_type": "file_write",
                "execution_time": 2.16
            }
        ]
        
        for i, pair in enumerate(test_pairs, 1):
            context_tracker.store_prompt_and_response(
                prompt=pair["prompt"],
                response=pair["response"],
                variable_map=variables_used,
                query_type=pair["query_type"],
                execution_time_seconds=pair["execution_time"]
            )
            print(f"âœ… Stored exchange {i}: {pair['query_type']} ({pair['execution_time']}s)")
        
        # Check what was stored
        stage_data = context_tracker.get_stage_summary("Test_Timing")
        
        print(f"\nğŸ“Š Timing Data Verification:")
        print(f"   ğŸ“ˆ Exchanges stored: {len(stage_data['prompt_response_pairs'])}")
        
        for i, pair in enumerate(stage_data['prompt_response_pairs'], 1):
            clock_time = pair.get('clock_time', 'N/A')
            execution_time = pair.get('execution_time_seconds', 'N/A') 
            query_type = pair.get('query_type', 'unknown')
            print(f"   ğŸ” Exchange {i}: {query_type}")
            print(f"      ğŸ• Clock time: {clock_time}")
            print(f"      â±ï¸ Execution: {execution_time}s")
        
        print(f"\nğŸ¯ Enhanced Timing Features:")
        print("âœ… Clock time: Shows when the query completed (HH:MM:SS)")
        print("âœ… Execution time: Shows how long the query took (seconds)")
        print("âœ… Precise timing: Rounded to 2 decimal places")
        print("âœ… All query types: Timing for multiple_choice, single_word, open, file_write")
        print("âœ… Context tracking: Timing data stored with each exchange")
        
        print(f"\nğŸ“‹ Report Display Changes:")
        print("ğŸ• Clock Time: Shows completion time with clock emoji")
        print("â±ï¸ Execution Time: Shows duration with stopwatch emoji") 
        print("ğŸ“ Prompt Size: Character count of prompt")
        print("ğŸ’¬ Response Size: Character count of response")
        print("ğŸ›ï¸ Congress Votes: Congressional oversight results")
        
        print(f"\nğŸ”§ Implementation Details:")
        print("â€¢ _execute_query() now times AI calls with time.time()")
        print("â€¢ context_tracker stores both clock_time and execution_time_seconds")
        print("â€¢ Report template shows ğŸ• HH:MM:SS and â±ï¸ X.XXs")
        print("â€¢ All 4 query types get timing automatically")
        print("â€¢ Precise logging shows execution time in console")
        
        return True
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run timing enhancements test"""
    print("ğŸ”§ GitLlama Timing Enhancements Test")
    print("=" * 70)
    
    success = test_timing_enhancements()
    
    print(f"\nğŸ¯ Test Summary:")
    print("=" * 70)
    if success:
        print("âœ… Timing enhancement tests PASSED!")
        print("ğŸ‰ Reports now show detailed timing information!")
        print("\nğŸ”§ What's Enhanced:")
        print("   â€¢ Clock time shows when each query completed")
        print("   â€¢ Execution time shows how long each query took")
        print("   â€¢ Automatic timing for all query types")
        print("   â€¢ Precise timing with 2 decimal places")
        print("   â€¢ Better performance monitoring capabilities")
        print("\nğŸ“ˆ User Benefits:")
        print("   â€¢ Monitor AI query performance")
        print("   â€¢ Identify slow queries for optimization")
        print("   â€¢ Track execution patterns over time")
        print("   â€¢ Debug timing-related issues")
        print("   â€¢ Professional timing metrics in reports")
    else:
        print("âŒ Some tests FAILED!")
        print("ğŸ”§ Check the implementation for errors")

if __name__ == "__main__":
    main()